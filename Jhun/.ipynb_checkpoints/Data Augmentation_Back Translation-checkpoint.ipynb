{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "624b4651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx==0.13.3 in /opt/conda/lib/python3.9/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: idna==2.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: hstspreload in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.12.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17415 sha256=068829d36a5933d9eb12fa8372da839f7ceb40b5a72d314fdc951f7eb71634ee\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/60/b3/27/d8aff3e2d5c2d0d97a117cdf0d5f13cd121e2c2b5fb49b55a0\n",
      "Successfully built googletrans\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 3.0.0\n",
      "    Uninstalling googletrans-3.0.0:\n",
      "      Successfully uninstalled googletrans-3.0.0\n",
      "Successfully installed googletrans-4.0.0rc1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "# 출처: https://intstorage.tistory.com/entry/파이썬을-이용한-구글-번역-API-활용법 [Interesting Story Storage:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e9bd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#번역\n",
    "def translate(sentence, type_='en'):\n",
    "    translator = Translator()\n",
    "    temp = translator.translate(sentence, dest=type_)\n",
    "    return translator.translate(temp.text, dest='ko').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca2255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_path =  os.getenv('HOME')+\"/aiffel/prj/train_bert.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cbac0efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=doc.txt --model_prefix=korean_spm --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: doc.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: doc.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 3950 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=845676\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1195\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 3950 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 42585 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 3950\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 51491\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 51491 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=24065 obj=11.6465 num_tokens=101375 num_tokens/piece=4.21255\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21411 obj=10.8989 num_tokens=101823 num_tokens/piece=4.75564\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1046622 Nov  7 09:12 korean_spm_large.model\r\n",
      "-rw-r--r-- 1 root root  844303 Nov  7 09:12 korean_spm_large.vocab\r\n",
      "-rw-r--r-- 1 root root  619503 Nov 21 03:55 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root  399760 Nov 21 03:55 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "#sentencepiece 학습\n",
    "with open('doc.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(df['text']))\n",
    "\n",
    "temp_file = 'doc.txt'\n",
    "vocab_size = 20000\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "\n",
    "!ls -l korean_spm*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f4361d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'야 너 휴대폰좀 빌려줘봐 안돼 지난달에도 휴대폰 요금이 너무 많이 나와서 엄마한테 혼났어 알 바야 닥치고 폰이나 내놔 폰 주면 또 결제하려고 그러지  응 게임 캐시 다떨어져서 충전좀 안돼 진짜 맞고 줄래 그냥 줄래 진짜 한번만 더 결제한거 들켰다가는 휴대폰 뺏길지도 몰라 아 씨 오늘따라 왜이렇게 말이 많아 당장 안내놔 제발 '"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원문\n",
    "df['text'][11]\n",
    "\n",
    "#국어 > 일어 > 영어 > 국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d61611cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네네 무슨 일 때문에 전화주셨나요 우리 애가 지우개 하나 훔친거 가지고 애들 앞에서 면박줬니 그런게 아니고 댁 내 따님분이 한 두번 훔친것도 아니고 여러번 훔친 게 확인 되어서 경찰부르는건 너무 심할거 같아서 꾸중 한 마디 한겁니다 아니 그 구멍가게 물건이 얼마 한다고 그러냐고 내 애가 맨날 훔친거 확실해 네 본 애들도 있고 확실합니다 됐고 내 애 울면서 하교 했어 이번 일 각오하는 게 좋을거야 너도 애 있다며 제 애까지 끌어들이지 마세요 내 애도 울면서 들어왔는데 그냥 못 넘어가 니 애 죽여버릴거야 이러지 마세요 저도 곤란합니다 단지 교육 차원에서 한 마디 했을 뿐이라고요 됐고 아 저기 네 애 보이네 죽여버릴테니까 니 아들 죽은거 보고 그때 반성해 남 애 함부로 울리면 어떻게 되는지\n",
      "\n",
      "내 아이가 지우개 앞에서 한 지우개를 훔쳐서 부러 뜨렸다. 나는 내 아이가 항상 훔치고 있다고 확신한다. 나는 4 명의 자녀가 있고, 나는 확실히 울고 거부했다.child. 이걸 준비하는 것이 좋다. 나는 그것을 버리지 않는다. 나는 또한 이것을하기가 어려웠다.\n",
      "\n",
      "직업을 위해 무엇을 부르 셨나요?내 아이는 지우개 앞에서 한 지우개를 훔쳐서 파산했습니다.나는 내 아이가 항상 훔치고 있다고 확신합니다.나는 4 명의 자녀가 있고 확실합니다.'확실히 아이를 울고 거부했습니다.이것을 준비하는 것이 좋습니다.나는 그것을 버리지 않습니다.나는 또한 이것을하기가 어려웠다.\n",
      "\n",
      "직업을 위해 무엇을 부르 셨나요?내 아이는 한 번의 지우개를 훔치고 아이들 앞에서 그것을 박살 냈습니다. 나는 내 아이가 항상 훔치고 있다고 확신합니다.나는 4 명의 아이들이 있고 확실합니다. 그리고 나는 아이들을 울고 해산했다고 확신합니다.이것에 대비하는 것이 좋습니다. 나는 그것을 버리지 않을 것입니다.나도 이것도하기가 어려웠다.\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "sentence = df['text'][12]\n",
    "print(sentence)\n",
    "print('')\n",
    "temp = translator.translate(sentence, dest='ja')\n",
    "temp = translator.translate(temp.text, dest='en')\n",
    "print(translator.translate(temp.text, dest='ko').text)\n",
    "print('')\n",
    "print(translate(sentence, type_='ja'))\n",
    "print('')\n",
    "print(translate(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "40b2c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이봐, 당신은 나에게 휴대폰을 빌려 줄 수 없다.지난 달, 나는 많은 휴대 전화를 가지고 있었기 때문에 어머니에게 SC였습니다.휴대폰이있을 수 있습니다.'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일어\n",
    "translate(df['text'][11], type_='ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "25aa017b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이봐, 당신은 나에게 휴대 전화를 빌려 줄 수 없다.지난 달, 나는 휴대 전화 요금이 많았 기 때문에 엄마에 의해 꾸짖 었습니다. 나는 휴대 전화를 받고 있을지도 모릅니다.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#영어\n",
    "translate(df['text'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b9cf5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'박인턴 올해 나이가 25이라고 했나 네 맞습니다  하 내가 이래서 사회 초년생은 들이려고 안했는데 네 저에게 무슨 문제라도 있으신가요  아니 박인턴이 우리 팀 들어오고 나서 한 번도 제대로 일이 풀린 적이 없지않나 그치만 제가 잘못을 해서 사회생활을 해봤어야 알지 뭐든 어설프고 참  이래서 어린친구들은 쓰고 싶지가 않아 대리님 죄송합니다 그치만 저 폐가 되지 않도록 열심히 업무에 집중하고 있습니다 요즘 애들은 말야 쫌만 힘들면 관두네 어렵네 투덜투덜 아주 회사가 우습지 놀러다니는 데도 아니고 저번에 한 번 전달드린 내용은 정말 제가 미처 전달받지 못했던 업무 내용이라 어려워서 그러니까 아니 애야 학생이야 어떻게 1부터 다 알려주나 그럴거면 학생을 데려다 일 시키지 쯧쯧'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "# print(df_shuffled)\n",
    "df_shuffled['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48cfcdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>text</th>\n",
       "      <th>bert_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "      <td>나는 당신이 스스로를 죽 이도록 격려하지 않습니다. 죄송합니다. 만약 내가 혼자 죽...</td>\n",
       "      <td>[CLS] 지금 너 스스로를 죽여달라고 애원하는 것인가? [SEP] 아닙니다. 죄송...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "      <td>경찰서입니다.9 : 40 나는 Mart에 폭발물을 설치할 것입니다. Gil -Don...</td>\n",
       "      <td>[CLS] 길동경찰서입니다. [SEP] 9시 40분 마트에 폭발물을 설치할거다. [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "      <td>당신은 정말로 귀엽다.</td>\n",
       "      <td>[CLS] 너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어. [SEP] 그만해....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "      <td>이봐,오고 싶어, 어떻게 생겼어요?나는 당신이 돈이 있다고 생각하지 않습니다.난 돈...</td>\n",
       "      <td>[CLS] 어이 거기 [SEP] 예?? [SEP] 너 말이야 너. 이리 오라고 [S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발...</td>\n",
       "      <td>[CLS] 저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation  \\\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...   \n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...   \n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...   \n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...   \n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  나는 당신이 스스로를 죽 이도록 격려하지 않습니다. 죄송합니다. 만약 내가 혼자 죽...   \n",
       "1  경찰서입니다.9 : 40 나는 Mart에 폭발물을 설치할 것입니다. Gil -Don...   \n",
       "2                                       당신은 정말로 귀엽다.   \n",
       "3  이봐,오고 싶어, 어떻게 생겼어요?나는 당신이 돈이 있다고 생각하지 않습니다.난 돈...   \n",
       "4  저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발...   \n",
       "\n",
       "                                           bert_text  \n",
       "0  [CLS] 지금 너 스스로를 죽여달라고 애원하는 것인가? [SEP] 아닙니다. 죄송...  \n",
       "1  [CLS] 길동경찰서입니다. [SEP] 9시 40분 마트에 폭발물을 설치할거다. [...  \n",
       "2  [CLS] 너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어. [SEP] 그만해....  \n",
       "3  [CLS] 어이 거기 [SEP] 예?? [SEP] 너 말이야 너. 이리 오라고 [S...  \n",
       "4  [CLS] 저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 ...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_label(data):\n",
    "    class_lst = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if data['class'][i] == class_lst[0]: #협박 대화\n",
    "            data['class'][i] = 0\n",
    "        elif data['class'][i] == class_lst[1]: #갈취 대화\n",
    "            data['class'][i] = 1\n",
    "        elif data['class'][i] == class_lst[2]: #직장 내 괴롭힘 대화\n",
    "            data['class'][i] = 2\n",
    "        elif data['class'][i] == class_lst[3]: #기타 괴롭힘 대화\n",
    "            data['class'][i] = 3\n",
    "    return data\n",
    "    \n",
    "\n",
    "def translate(sentence, type_='en'):\n",
    "    translator = Translator()\n",
    "    temp = translator.translate(sentence, dest=type_)\n",
    "    return translator.translate(temp.text, dest='ko').text\n",
    "\n",
    "def back_translation(data):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True) #데이터의 위치를 셔플\n",
    "\n",
    "    for i in range(int(len(data)/10)):\n",
    "        if i // 2 == 0:\n",
    "            data['text'][i] = translate(data['text'][i]) #짝수일경우 kr > en > kr\n",
    "        elif i // 2 ==1:\n",
    "            data['text'][i] = translate(data['text'][i], type_='ja') #홀수일경우 kr > ja > kr \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# back_translation(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2b914dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 551, 3082, 34, 611, 28, 574, 16355, 602, 627, 2346, 3586, 26, 3586, 2076, 7, 393, 634, 16534, 31, 6708, 149, 2654, 9224, 1055, 299, 51, 34, 926, 926, 3120, 3586, 2076, 551, 1744, 19, 89, 64, 7895, 6708, 160, 25, 14880, 160, 144, 352, 19, 1057, 5475, 3586]\n",
      "['▁나는', '▁당신이', '▁스스로', '를', '▁죽', '▁이', '도록', '▁격', '려', '하지', '▁않습니다', '.', '▁죄송합니다', '.', '▁만약', '▁내가', '▁혼자', '▁죽을', '▁것이라', '면', ',', '▁나는', '▁혼자서', '▁죽었', '▁기', '▁때문에', '▁우리', '를', '▁죽이고', '▁죽이고', '▁싶습니다', '.', '▁만약', '▁당신이', '▁그것', '을', '▁할', '▁수', '▁없다면', ',', '▁당신', '은', '▁당신과', '▁당신', '의', '▁가족', '을', '▁죽일', '▁것입니다', '.']\n",
      "나는 당신이 스스로를 죽 이도록 격려하지 않습니다. 죄송합니다. 만약 내가 혼자 죽을 것이라면, 나는 혼자서 죽었 기 때문에 우리를 죽이고 죽이고 싶습니다. 만약 당신이 그것을 할 수 없다면, 당신은 당신과 당신의 가족을 죽일 것입니다.\n"
     ]
    }
   ],
   "source": [
    "#sp 예시\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds(df['text'][0])\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces(df['text'][0],1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2818c000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가', '가까스로', '가령', '각', '각각']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 lst 호출\n",
    "stopwords = []\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line.replace('\\n',''))#print(line.strip())\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fe89e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#back translation 데이터를 불용어 및 토큰화 진행\n",
    "\n",
    "def sp_tokenize(data): \n",
    "    train_data = []\n",
    "    data['text'] = data['text'].str.replace(\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #특수문자 제거\n",
    "    \n",
    "    for sentence in tqdm(data['text']): #불용어 제거 및 리스트 생성\n",
    "        tokenized_sentence = s.SampleEncodeAsPieces(sentence,1, 0.0)\n",
    "        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "        train_data.append(stopwords_removed_sentence)\n",
    "    \n",
    "    tokenizer_sp = Tokenizer()\n",
    "    tokenizer_sp.fit_on_texts(train_data)\n",
    "        \n",
    "    vocab_size = len(tokenizer_sp.word_index)\n",
    "    train_data = tokenizer_sp.texts_to_sequences(train_data)\n",
    "    \n",
    "    max_lan = max(len(review) for review in X_mec_train_sequences)\n",
    "    \n",
    "    train_data = pad_sequences(train_data, maxlen = max_lan)\n",
    "    \n",
    "    return train_data, vocab_size, tokenizer_sp\n",
    "\n",
    "def y_categori(ydata):\n",
    "    ydata = class_label(ydata)\n",
    "    y=[] \n",
    "    for i in range(len(ydata)):\n",
    "        y.append(ydata['class'][i])\n",
    "    y_data = to_categorical(y)\n",
    "    \n",
    "    return y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e603991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7584/2772842773.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #특수문자 제거\n",
      "100%|██████████| 3950/3950 [00:03<00:00, 1314.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# data augmentation 하려면 1epoch당 하기 파일 한번씩 돌려서 데이터셋 재생성\n",
    "def train_data_set(data,size_=0.1, random_state=None):\n",
    "    data_set = back_translation(data) #data augmentation\n",
    "    X_data, vocab_size, tokenizer_sp = sp_tokenize(data_set)\n",
    "    y_data = y_categori(df)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size= size_, shuffle=True, random_state=34)\n",
    "    \n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_data_set(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
