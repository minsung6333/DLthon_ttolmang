{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e36829bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx==0.13.3 in /opt/conda/lib/python3.9/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: idna==2.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: hstspreload in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.12.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17415 sha256=068829d36a5933d9eb12fa8372da839f7ceb40b5a72d314fdc951f7eb71634ee\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/60/b3/27/d8aff3e2d5c2d0d97a117cdf0d5f13cd121e2c2b5fb49b55a0\n",
      "Successfully built googletrans\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 3.0.0\n",
      "    Uninstalling googletrans-3.0.0:\n",
      "      Successfully uninstalled googletrans-3.0.0\n",
      "Successfully installed googletrans-4.0.0rc1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "# 출처: https://intstorage.tistory.com/entry/파이썬을-이용한-구글-번역-API-활용법 [Interesting Story Storage:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e17383cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#번역\n",
    "def translate(sentence, type_='en'):\n",
    "    translator = Translator()\n",
    "    temp = translator.translate(sentence, dest=type_)\n",
    "    return translator.translate(temp.text, dest='ko').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc8066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_path =  os.getenv('HOME')+\"/aiffel/DLthon_data/train_bert.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca716c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=doc.txt --model_prefix=korean_spm --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: doc.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: doc.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 3950 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=846088\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1188\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 3950 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 42612 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 3950\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 51506\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 51506 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=24071 obj=11.6456 num_tokens=101396 num_tokens/piece=4.21237\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21412 obj=10.8981 num_tokens=101842 num_tokens/piece=4.7563\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 619618 Nov 21 06:50 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 399337 Nov 21 06:50 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "#sentencepiece 학습\n",
    "with open('doc.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(df['text']))\n",
    "\n",
    "temp_file = 'doc.txt'\n",
    "vocab_size = 20000\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "\n",
    "!ls -l korean_spm*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73cd2455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이대리 오늘 도대체 뭘한거야 창고정리도 하고 바닥도 청소했습니다 이게 한거니 네 열심히 했습니다 왜 이건 복사 안해놨니 아까 하라고 안하셨는데요 내가 아까 말했는데 너가 못들었어 귀 이니 아닙니다 지금 당장 하겠습니다 물도 새로 받아와 이걸 먹으라고 한거니 죄송합니다'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원문\n",
    "df['text'][11]\n",
    "\n",
    "#국어 > 일어 > 영어 > 국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e65153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네네 무슨 일 때문에 전화주셨나요 우리 애가 지우개 하나 훔친거 가지고 애들 앞에서 면박줬니 그런게 아니고 댁 내 따님분이 한 두번 훔친것도 아니고 여러번 훔친 게 확인 되어서 경찰부르는건 너무 심할거 같아서 꾸중 한 마디 한겁니다 아니 그 구멍가게 물건이 얼마 한다고 그러냐고 내 애가 맨날 훔친거 확실해 네 본 애들도 있고 확실합니다 됐고 내 애 울면서 하교 했어 이번 일 각오하는 게 좋을거야 너도 애 있다며 제 애까지 끌어들이지 마세요 내 애도 울면서 들어왔는데 그냥 못 넘어가 니 애 죽여버릴거야 이러지 마세요 저도 곤란합니다 단지 교육 차원에서 한 마디 했을 뿐이라고요 됐고 아 저기 네 애 보이네 죽여버릴테니까 니 아들 죽은거 보고 그때 반성해 남 애 함부로 울리면 어떻게 되는지\n",
      "\n",
      "내 아이가 지우개 앞에서 한 지우개를 훔쳐서 부러 뜨렸다. 나는 내 아이가 항상 훔치고 있다고 확신한다. 나는 4 명의 자녀가 있고, 나는 확실히 울고 거부했다.child. 이걸 준비하는 것이 좋다. 나는 그것을 버리지 않는다. 나는 또한 이것을하기가 어려웠다.\n",
      "\n",
      "직업을 위해 무엇을 부르 셨나요?내 아이는 지우개 앞에서 한 지우개를 훔쳐서 파산했습니다.나는 내 아이가 항상 훔치고 있다고 확신합니다.나는 4 명의 자녀가 있고 확실합니다.'확실히 아이를 울고 거부했습니다.이것을 준비하는 것이 좋습니다.나는 그것을 버리지 않습니다.나는 또한 이것을하기가 어려웠다.\n",
      "\n",
      "직업을 위해 무엇을 부르 셨나요?내 아이는 한 번의 지우개를 훔치고 아이들 앞에서 그것을 박살 냈습니다. 나는 내 아이가 항상 훔치고 있다고 확신합니다.나는 4 명의 아이들이 있고 확실합니다. 그리고 나는 아이들을 울고 해산했다고 확신합니다.이것에 대비하는 것이 좋습니다. 나는 그것을 버리지 않을 것입니다.나도 이것도하기가 어려웠다.\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "sentence = df['text'][12]\n",
    "print(sentence)\n",
    "print('')\n",
    "temp = translator.translate(sentence, dest='ja')\n",
    "temp = translator.translate(temp.text, dest='en')\n",
    "print(translator.translate(temp.text, dest='ko').text)\n",
    "print('')\n",
    "print(translate(sentence, type_='ja'))\n",
    "print('')\n",
    "print(translate(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b6b3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lee Dae -Ri 오늘 뭐해?창고를 청소하고 바닥을 청소했습니다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일어\n",
    "translate(df['text'][11], type_='ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "328b11f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lee Dae -Ri 오늘 대체 뭐해?창고를 청소하고 바닥을 청소했습니다.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#영어\n",
    "translate(df['text'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4abea4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'시끄러워서 찾아왔는데 진짜 말로 하면 안 듣네 맞아야하나 조심하고있어요 조심하는데 소리가 왜 이렇게 나 몽둥이로 패버리고싶네 여기서 나는 소리가 아닌가보죠 여기가 아니면 어디서 나 죽고 싶어 당신 좀더 주의할게요 이만 내려가주세요 내려가 한번만 더 시끄럽게 하면 불질러버릴거야 주의할게요 다음부터는 직접 말고 유선으로 연락하세요 뭐 이거 말하는거좀봐 유선이 아니라 그냥 불질러버릴거야 네네 조심할게요'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "# print(df_shuffled)\n",
    "df_shuffled['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704a2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_label(data):\n",
    "    class_lst = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if data['class'][i] == class_lst[0]: #협박 대화\n",
    "            data['class'][i] = 0\n",
    "        elif data['class'][i] == class_lst[1]: #갈취 대화\n",
    "            data['class'][i] = 1\n",
    "        elif data['class'][i] == class_lst[2]: #직장 내 괴롭힘 대화\n",
    "            data['class'][i] = 2\n",
    "        elif data['class'][i] == class_lst[3]: #기타 괴롭힘 대화\n",
    "            data['class'][i] = 3\n",
    "    return data\n",
    "    \n",
    "\n",
    "def translate(sentence, type_='en'):\n",
    "    translator = Translator()\n",
    "    temp = translator.translate(sentence, dest=type_)\n",
    "    return translator.translate(temp.text, dest='ko').text\n",
    "\n",
    "def back_translation(data):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True) #데이터의 위치를 셔플\n",
    "\n",
    "    for i in range(int(len(data)/10)):\n",
    "        if i // 2 == 0:\n",
    "            data['text'][i] = translate(data['text'][i]) #짝수일경우 kr > en > kr\n",
    "        elif i // 2 ==1:\n",
    "            data['text'][i] = translate(data['text'][i], type_='ja') #홀수일경우 kr > ja > kr \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# back_translation(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a2388d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 4, 3083, 34, 689, 1517, 203, 11427, 15253, 129, 26, 640, 49, 31, 392, 4608, 51, 114, 5389, 38, 3348, 1129, 12220, 71, 27, 2389, 579, 23, 68, 1664, 93, 5936, 93, 1844, 8, 352, 19, 689, 739, 26, 68, 1664, 3784, 12, 4708, 32, 72, 2627, 3881, 31, 5018, 8, 4843, 8115, 603, 2627, 52, 726, 256, 1841, 41, 21, 15464, 976, 28, 145, 523, 48, 1841]\n",
      "['▁지금', '▁너', '▁스스로', '를', '▁죽여', '달라고', '▁애', '원하는', '▁것인가', '▁아닙니다', '▁죄송합니다', '▁죽을', '▁거', '면', '▁혼자', '▁죽지', '▁우리', '까지', '▁사건', '에', '▁휘', '말', '리게', '▁해', '▁진짜', '▁죽여버리고', '▁싶', '게', '▁정말', '▁잘못했습니다', '▁너가', '▁선택해', '▁너가', '▁죽을래', '▁네', '▁가족', '을', '▁죽여', '줄까', '▁죄송합니다', '▁정말', '▁잘못했습니다', '▁너에게', '는', '▁선택권', '이', '▁없어', '▁선택', '▁못한다', '면', '▁너와', '▁네', '▁가족까지', '▁모조리', '▁죽여버릴거야', '▁선택', '▁못', '하겠습니다', '▁한번만', '▁도와주세요', '▁그냥', '▁다', '▁죽여버려야겠', '군', '▁이', '의', '▁없지', '▁제발', '▁도와주세요']\n",
      "지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요\n"
     ]
    }
   ],
   "source": [
    "#sp 예시\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds(df['text'][0])\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces(df['text'][0],1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e4b93d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가', '가까스로', '가령', '각', '각각']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 lst 호출\n",
    "file_path = os.getenv('HOME')+\"/aiffel/DLthon_data/stopwords.txt\"\n",
    "\n",
    "stopwords = []\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line.replace('\\n',''))#print(line.strip())\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d914a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#back translation 데이터를 불용어 및 토큰화 진행\n",
    "\n",
    "def sp_tokenize(data): \n",
    "    train_data = []\n",
    "    data['text'] = data['text'].str.replace(\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #특수문자 제거\n",
    "    \n",
    "    for sentence in tqdm(data['text']): #불용어 제거 및 리스트 생성\n",
    "        tokenized_sentence = s.SampleEncodeAsPieces(sentence,1, 0.0)\n",
    "        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "        train_data.append(stopwords_removed_sentence)\n",
    "    \n",
    "    tokenizer_sp = Tokenizer()\n",
    "    tokenizer_sp.fit_on_texts(train_data)\n",
    "        \n",
    "    vocab_size = len(tokenizer_sp.word_index)\n",
    "    train_data = tokenizer_sp.texts_to_sequences(train_data)\n",
    "    \n",
    "    max_lan = max(len(review) for review in X_mec_train_sequences)\n",
    "    \n",
    "    train_data = pad_sequences(train_data, maxlen = max_lan)\n",
    "    \n",
    "    return train_data, vocab_size, tokenizer_sp\n",
    "\n",
    "def y_categori(ydata):\n",
    "    ydata = class_label(ydata)\n",
    "    y=[] \n",
    "    for i in range(len(ydata)):\n",
    "        y.append(ydata['class'][i])\n",
    "    y_data = to_categorical(y)\n",
    "    \n",
    "    return y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85ad1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95/3364750417.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'][i] = translate(data['text'][i]) #짝수일경우 kr > en > kr\n",
      "/tmp/ipykernel_95/3364750417.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'][i] = translate(data['text'][i], type_='ja') #홀수일경우 kr > ja > kr\n",
      "/tmp/ipykernel_95/2772842773.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #특수문자 제거\n",
      "100%|██████████| 3950/3950 [00:03<00:00, 1301.93it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_mec_train_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_95/2210162337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_95/2210162337.py\u001b[0m in \u001b[0;36mtrain_data_set\u001b[0;34m(data, size_, random_state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_categori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_95/2772842773.py\u001b[0m in \u001b[0;36msp_tokenize\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_sp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmax_lan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_mec_train_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_lan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_mec_train_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# data augmentation 하려면 1epoch당 하기 파일 한번씩 돌려서 데이터셋 재생성\n",
    "def train_data_set(data,size_=0.1, random_state=None):\n",
    "    data_set = back_translation(data) #data augmentation\n",
    "    X_data, vocab_size, tokenizer_sp = sp_tokenize(data_set)\n",
    "    y_data = y_categori(df)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size= size_, shuffle=True, random_state=34)\n",
    "    \n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_data_set(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2573cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
